{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eNq4lCSSQWn"
      },
      "source": [
        "**Instituto Tecnológico de Costa Rica - TEC**\n",
        "\n",
        "***Inteligencia Artificial***\n",
        "\n",
        "*Docente: Kenneth Obando Rodríguez*\n",
        "\n",
        "---\n",
        "# Trabajo Corto 3: Árboles de Decisión\n",
        "---\n",
        "Estudiantes:\n",
        "- Ariel Leyva c.2022175018\n",
        "\n",
        "Link del Cuaderno (recuerde configurar el acceso a público):\n",
        "\n",
        "    \n",
        "- [Link de su respuesta](https://github.com/hart-venus/tc4-ia/)\n",
        "\n",
        "    **Nota:** Este trabajo tiene como objetivo promover la comprensión de la materia y su importancia en la elección de algoritmos. Los alumnos deben evitar copiar y pegar directamente información de fuentes externas, y en su lugar, demostrar su propio análisis y comprensión.\n",
        "\n",
        "### Entrega\n",
        "Debe entregar un archivo comprimido por el TecDigital, incluyendo un documento pdf con los resultados de los experimentos y pruebas. La fecha de entrega es el miércoles 1 de mayo a las 10:00 p.m."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmAGdF_-Xhj1"
      },
      "source": [
        "Instrucciones:\n",
        "\n",
        "Las alternativas se rifarán en clase utilizando números aleatorios. Deberá realizar la asignación propuesta. Si realiza ambos ejercicios, recibirá 20 puntos en **la nota porcentual de la actividad**, para aplicar a la totalidad de los puntos extra es necesario que ambas actividades se completen al 100%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onCmI27E3Jit"
      },
      "source": [
        "## Actividad - Taller\n",
        "\n",
        "1. Cree una clase nodo con atributos necesarios para un árbol de decisión: feature, umbral, gini, cantidad_muestras, valor, izquierda, derecha\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "class Nodo:\n",
        "    def __init__(self, feature=None, umbral=None, gini=None, cantidad_muestras=None, valor=None, izquierda=None, derecha=None):\n",
        "        self.feature = feature  # Índice de la característica (feature) para la división\n",
        "        self.umbral = umbral  # Valor umbral para la división\n",
        "        self.gini = gini  # Impureza de Gini en el nodo\n",
        "        self.cantidad_muestras = cantidad_muestras  # Cantidad de muestras en el nodo\n",
        "        self.valor = valor  # Valor de predicción (para nodos hoja)\n",
        "        self.izquierda = izquierda  # Nodo hijo izquierdo\n",
        "        self.derecha = derecha  # Nodo hijo derecho\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explicación de los atributos:\n",
        "\n",
        "- feature: Representa el índice de la característica (feature) utilizada para la división en el nodo. Es None para los nodos hoja.\n",
        "- umbral: Representa el valor umbral utilizado para la división en el nodo. Las muestras con valores menores o iguales al umbral se envían al nodo hijo izquierdo, mientras que las muestras con valores mayores se envían al nodo hijo derecho. Es None para los nodos hoja.\n",
        "- gini: Representa la impureza de Gini en el nodo, que mide la calidad de la división. Un valor de Gini más bajo indica una división más pura.\n",
        "- cantidad_muestras: Representa la cantidad de muestras que llegan al nodo durante el entrenamiento del árbol.\n",
        "- valor: Representa el valor de predicción para los nodos hoja. En un árbol de clasificación, suele ser la clase mayoritaria entre las muestras que llegan al nodo hoja. En un árbol de regresión, puede ser el promedio de los valores de las muestras en el nodo hoja.\n",
        "- izquierda: Representa el nodo hijo izquierdo, que contiene las muestras con valores menores o iguales al umbral de división. Es None para los nodos hoja.\n",
        "- derecha: Representa el nodo hijo derecho, que contiene las muestras con valores mayores al umbral de división. Es None para los nodos hoja.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Crea una clase que implementa un árbol de decisión, utilice las funciones presentadas en clase, además incluya los siguientes hyperparámetros:\n",
        "   - max_depth: Cantidad máxima de variables que se pueden explorar\n",
        "   - min_split_samples: Cantidad mínima de muestras que deberá tener un nodo para poder ser dividido\n",
        "   - criterio: función que se utilizará para calcular la impuridad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ArbolDecision:\n",
        "    def __init__(self, max_depth=None, min_split_samples=2, criterio='gini'):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_split_samples = min_split_samples\n",
        "        self.criterio = criterio\n",
        "        self.root = None\n",
        "\n",
        "    def _calcular_gini(self, y):\n",
        "        _, counts = np.unique(y, return_counts=True) # cuenta las ocurrencias de cada clase\n",
        "        proporciones = counts / len(y) # calcula la proporción de cada clase\n",
        "        gini = 1 - np.sum(proporciones ** 2) # calcula la impureza de Gini\n",
        "        return gini \n",
        "\n",
        "    def _dividir(self, X, y, feature, umbral):\n",
        "        izquierda_indices = X[:, feature] <= umbral\n",
        "        derecha_indices = X[:, feature] > umbral # Divide basado en los umbrales\n",
        "        X_izquierda, y_izquierda = X[izquierda_indices], y[izquierda_indices]\n",
        "        X_derecha, y_derecha = X[derecha_indices], y[derecha_indices]\n",
        "        return X_izquierda, y_izquierda, X_derecha, y_derecha\n",
        "\n",
        "    def _construir_arbol(self, X, y, depth=0):\n",
        "        cantidad_muestras = len(y)\n",
        "        if cantidad_muestras < self.min_split_samples or depth == self.max_depth: # caso base\n",
        "            valor = np.bincount(y).argmax()\n",
        "            return Nodo(cantidad_muestras=cantidad_muestras, valor=valor)\n",
        "\n",
        "        mejor_feature, mejor_umbral, mejor_gini = None, None, float('inf')\n",
        "        for feature in range(X.shape[1]):\n",
        "            umbrales = np.unique(X[:, feature])\n",
        "            for umbral in umbrales: # construye el arbol por cada umbral\n",
        "                X_izquierda, y_izquierda, X_derecha, y_derecha = self._dividir(X, y, feature, umbral)\n",
        "                if len(y_izquierda) > 0 and len(y_derecha) > 0: # si hay muestras en ambos nodos hijos\n",
        "                    gini_izquierda = self._calcular_gini(y_izquierda)\n",
        "                    gini_derecha = self._calcular_gini(y_derecha)\n",
        "                    gini = (len(y_izquierda) * gini_izquierda + len(y_derecha) * gini_derecha) / cantidad_muestras\n",
        "                    if gini < mejor_gini:\n",
        "                        mejor_feature, mejor_umbral, mejor_gini = feature, umbral, gini\n",
        "\n",
        "        if mejor_feature is None: # si no se puede dividir, caso base\n",
        "            valor = np.bincount(y).argmax()\n",
        "            return Nodo(cantidad_muestras=cantidad_muestras, valor=valor)\n",
        "\n",
        "        X_izquierda, y_izquierda, X_derecha, y_derecha = self._dividir(X, y, mejor_feature, mejor_umbral)\n",
        "        nodo_izquierdo = self._construir_arbol(X_izquierda, y_izquierda, depth + 1) # crear hijos recursivamente\n",
        "        nodo_derecho = self._construir_arbol(X_derecha, y_derecha, depth + 1)\n",
        "\n",
        "        return Nodo(feature=mejor_feature, umbral=mejor_umbral, gini=mejor_gini,\n",
        "                    cantidad_muestras=cantidad_muestras, izquierda=nodo_izquierdo, derecha=nodo_derecho)\n",
        "\n",
        "    def fit(self, X, y): # metodo publico para entrenar el modelo\n",
        "        self.root = self._construir_arbol(X, y)\n",
        "\n",
        "    def _predecir(self, x, nodo): \n",
        "        if nodo.valor is not None:\n",
        "            return nodo.valor\n",
        "        if x[nodo.feature] <= nodo.umbral:\n",
        "            return self._predecir(x, nodo.izquierda)\n",
        "        else:\n",
        "            return self._predecir(x, nodo.derecha)\n",
        "\n",
        "    def predict(self, X): # metodo publico para hacer predicciones\n",
        "        return np.array([self._predecir(x, self.root) for x in X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Divida los datos en los conjuntos tradicionales de entrenamiento y prueba, de forma manual, sin utilizar las utilidades de sklearn (puede utilizar índices de Numpy o Pandas)\n",
        "\n",
        "4. Implemente una función que se llame `validacion_cruzada` que entrene $k$ modelos y reporte las métricas obtenidas:\n",
        "  a. Divida el conjunto de entrenamiento en $k$ subconjuntos excluyentes\n",
        "  b. Para cada uno de los $k$ modelos, utilice un subconjunto como validación\n",
        "  c. Reporte la media y la desviación estándar para cada una de las métricas, todo debe realizarse solo usando Numpy:\n",
        "    - Accuracy\n",
        "    - Precision\n",
        "    - Recall\n",
        "    - F1\n",
        "  \n",
        "5. Entrene 10 combinaciones distintas de parámetros para su implementación de Arbol de Decisión y utilizando su implementación de `validacion_cruzada`.\n",
        "\n",
        "6. Utilizando los resultados obtenidos analice cuál y porqué es el mejor modelo para ser usado en producción.\n",
        "\n",
        "7. Compruebe las métricas usando el conjunto de prueba y analice el resultado\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_9m6Jqr25fi"
      },
      "source": [
        "## Rúbrica para la Implementación de un Árbol de Decisión\n",
        "\n",
        "**Nota: Esta rúbrica se basa en la calidad de la implementación y los resultados obtenidos, no en la cantidad de código.**\n",
        "\n",
        "**1. Creación de la Clase Nodo (10 puntos)**\n",
        "\n",
        "- [ ] Se crea una clase `Nodo` con los atributos mencionados en las especificaciones (feature, umbral, gini, cantidad_muestras, valor, izquierda, derecha).\n",
        "- [ ] Los atributos se definen correctamente y se asignan de manera apropiada.\n",
        "\n",
        "**2. Creación de la Clase Árbol de Decisión (20 puntos)**\n",
        "\n",
        "- [ ] Se crea una clase que implementa un árbol de decisión.\n",
        "- [ ] La clase utiliza las funciones presentadas en el cuaderno.\n",
        "- [ ] Se implementan los hyperparámetros solicitados (max_depth, min_split_samples, criterio).\n",
        "- [ ] La clase es capaz de entrenar un árbol de decisión con los hyperparámetros especificados.\n",
        "\n",
        "**3. División de Datos (10 puntos)**\n",
        "\n",
        "- [ ] Los datos se dividen en conjuntos de entrenamiento y prueba de forma manual.\n",
        "- [ ] Se utiliza Numpy o Pandas para realizar esta división.\n",
        "- [ ] Se garantiza que los conjuntos sean excluyentes.\n",
        "\n",
        "**4. Implementación de Validación Cruzada (20 puntos)**\n",
        "\n",
        "- [ ] Se implementa la función `validacion_cruzada` correctamente.\n",
        "- [ ] Los datos de entrenamiento se dividen en k subconjuntos excluyentes.\n",
        "- [ ] Se entrena y evalúa un modelo para cada subconjunto de validación.\n",
        "- [ ] Se calculan y reportan las métricas de accuracy, precision, recall y F1.\n",
        "- [ ] Se calcula la media y la desviación estándar de estas métricas.\n",
        "\n",
        "**5. Entrenamiento de Modelos (20 puntos)**\n",
        "\n",
        "- [ ] Se entrenan 10 combinaciones distintas de parámetros para el árbol de decisión.\n",
        "- [ ] Cada combinación se entrena utilizando la función `validacion_cruzada`.\n",
        "- [ ] Los resultados de las métricas se registran adecuadamente.\n",
        "\n",
        "**6. Análisis de Modelos (10 puntos)**\n",
        "\n",
        "- [ ] Se analizan los resultados obtenidos y se selecciona el mejor modelo para ser utilizado en producción.\n",
        "- [ ] Se proporciona una justificación clara y fundamentada sobre por qué se eligió ese modelo.\n",
        "\n",
        "**7. Prueba en el Conjunto de Prueba (10 puntos)**\n",
        "\n",
        "- [ ] Se comprueban las métricas del modelo seleccionado utilizando el conjunto de prueba.\n",
        "- [ ] Se analizan los resultados y se comentan las conclusiones.\n",
        "\n",
        "**General (10 puntos)**\n",
        "\n",
        "- [ ] El código se documenta de manera adecuada, incluyendo comentarios que expliquen las secciones clave.\n",
        "- [ ] El código se ejecuta sin errores y sigue buenas prácticas de programación.\n",
        "- [ ] La presentación de los resultados es clara y fácil de entender.\n",
        "- [ ] Se cumple con todos los requisitos y las especificaciones proporcionadas.\n",
        "\n",
        "**Puntuación Total: 100 puntos**\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
